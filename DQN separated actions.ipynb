{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import seaborn as sb\n",
        "import itertools\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "\n",
        "np.seterr(invalid='raise')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueEstimator:\n",
        "  def __init__(self, layers, action_space):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    self.action_space = action_space\n",
        "\n",
        "    self.layers = []\n",
        "\n",
        "    for i in range(len(layers) - 1):\n",
        "      weights = np.random.normal(scale=np.sqrt(2/layers[i]), size=(layers[i+1], layers[i]))\n",
        "      biases = np.zeros((layers[i+1], 1))\n",
        "      self.layers.append((weights, biases, 'leaky relu'))\n",
        "\n",
        "    #output layer\n",
        "    weights = np.random.normal(scale=np.sqrt(2/layers[-1]), size=(1, layers[-1]))\n",
        "    biases = np.zeros((1, 1))\n",
        "    self.layers.append((weights, biases, 'linear'))\n",
        "\n",
        "\n",
        "  def fit(self, states, target, learning_rate=.01, return_grads=False, learning=True):\n",
        "      \"\"\"\n",
        "      \"\"\"\n",
        "      pred, caches = self.predict(states)\n",
        "      dA = MSE_grad(target, pred)\n",
        "      #dA_norm = np.linalg.norm(dA)\n",
        "      #dA = dA / dA_norm if (dA_norm>1) else dA\n",
        "\n",
        "      m = states.shape[1]\n",
        "\n",
        "      grads = []\n",
        "      for i in reversed(range(len(self.layers))):\n",
        "          A_prev, W, b, Z = caches[i]\n",
        "\n",
        "          #Activation function derivatives\n",
        "          if self.layers[i][2] == 'relu':\n",
        "              dZ = dA\n",
        "              dZ[Z <= 0] = 0\n",
        "          elif self.layers[i][2] == 'leaky relu':\n",
        "              dZ = np.where(Z > 0, dA, dA * 0.01)\n",
        "          elif self.layers[i][2] == 'linear':\n",
        "              dZ = dA\n",
        "          elif self.layers[i][2] == 'sigmoid':\n",
        "              raise NotImplementedError('Sigmoid derivitive not implemented.')\n",
        "\n",
        "          #Weight function derivatives\n",
        "          dW = np.dot(dZ, A_prev.T)/m\n",
        "          dB = np.sum(dZ, axis = 1, keepdims=True)/m\n",
        "          dA = np.dot(self.layers[i][0].T, dZ)\n",
        "\n",
        "          \"\"\"grads.append(np.sum(dB))\n",
        "          grads.append(np.sum(dW))\"\"\"\n",
        "\n",
        "\n",
        "          #Gradient Updates\n",
        "          #print(f\"\"\"Weight grad: {-(learning_rate*dW)}\n",
        "          #Bias grad: {-(learning_rate*dB)}\"\"\")\n",
        "          if learning:\n",
        "              self.layers[i] = (self.layers[i][0]-(learning_rate*dW), self.layers[i][1]-(learning_rate*dB), self.layers[i][2])\n",
        "\n",
        "      if return_grads:\n",
        "          return dA, np.flip(np.array(grads))\n",
        "      return dA\n",
        "\n",
        "\n",
        "  def predict(self, X, grad_check_epsilon=0.0, grad_check_param=None, print_layer_outputs=False):\n",
        "    \"\"\"\n",
        "    Forward-propogates an input through the model to create a prediction on the value of each action.\n",
        "\n",
        "    Args:\n",
        "    X - Array of all inputs\n",
        "    \"\"\"\n",
        "    caches = []\n",
        "    if (grad_check_param == None):\n",
        "        for i, l in enumerate(self.layers):\n",
        "            A = X.copy()\n",
        "            X = np.dot(l[0], X) + l[1]\n",
        "            cache = (A, l[0].copy(), l[1].copy(), X.copy())\n",
        "            caches.append(cache)\n",
        "\n",
        "            if l[2] == 'relu':\n",
        "                np.maximum(X, 0, out=X)\n",
        "            elif l[2] == 'leaky relu':\n",
        "                X = np.where(X > 0, X, X * 0.01)\n",
        "            elif l[2] == 'linear':\n",
        "                pass\n",
        "            elif l[2] == 'sigmoid':\n",
        "                X = 1/(1+np.exp(-X))\n",
        "\n",
        "            if print_layer_outputs:\n",
        "                print(f\"Layer {i}\", X)\n",
        "        return X, caches\n",
        "\n",
        "    #grad check version\n",
        "    i=0\n",
        "    for l in self.layers:\n",
        "        W = l[0] if (i!=grad_check_param) else l[0]+grad_check_epsilon\n",
        "        b = l[1] if (i+1!=grad_check_param) else l[1]+grad_check_epsilon\n",
        "\n",
        "        A = X.copy()\n",
        "        X = np.dot(W, X) + b\n",
        "        cache = (A, l[0].copy(), l[1].copy(), X.copy())\n",
        "        caches.append(cache)\n",
        "\n",
        "        if l[2] == 'relu':\n",
        "            np.maximum(X, 0, out=X)\n",
        "        elif l[2] == 'leaky relu':\n",
        "            X = np.where(X > 0, X, X * 0.01)\n",
        "        elif l[2] == 'linear':\n",
        "            pass\n",
        "        elif l[2] == 'sigmoid':\n",
        "            X = 1/(1+np.exp(-X))\n",
        "\n",
        "        i+=2\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "  def grad_check(self, X, Y, epsilon=1e-7):\n",
        "      estimates = []\n",
        "      i=0\n",
        "      for l in enumerate(self.layers):\n",
        "          J_plus_W = float(mean_squared_error(Y, self.predict(X, grad_check_epsilon=epsilon, grad_check_param=i), ax=None))\n",
        "          J_minus_W = float(mean_squared_error(Y, self.predict(X, grad_check_epsilon=-epsilon, grad_check_param=i), ax=None))\n",
        "          J_W = (J_plus_W - J_minus_W) / (2*epsilon)\n",
        "          estimates.append(J_W)\n",
        "\n",
        "          J_plus_b = float(mean_squared_error(Y, self.predict(X, grad_check_epsilon=epsilon, grad_check_param=i+1), ax=None))\n",
        "          J_minus_b = float(mean_squared_error(Y, self.predict(X, grad_check_epsilon=-epsilon, grad_check_param=i+1), ax=None))\n",
        "          J_b = (J_plus_b - J_minus_b) / (2*epsilon)\n",
        "          estimates.append(J_b)\n",
        "\n",
        "          i+=2\n",
        "      estimates = np.array(estimates)\n",
        "\n",
        "      _, grads = self.fit(X, Y, return_grads=True, learning=False)\n",
        "\n",
        "      numerator = np.linalg.norm(grads - estimates)\n",
        "      denominator = np.linalg.norm(grads) + np.linalg.norm(estimates)\n",
        "      difference = numerator / denominator\n",
        "\n",
        "      return difference, estimates, grads\n",
        "\n",
        "\n",
        "  def addActionstoState(self, states, a=None):\n",
        "      if a == None:\n",
        "          actions = np.zeros((self.action_space, self.action_space))\n",
        "          for i in range(self.action_space):\n",
        "              actions[i, i] = 1\n",
        "          actions = np.tile(actions, states.shape[1])\n",
        "\n",
        "          states = np.repeat(states, self.action_space, axis=1)\n",
        "\n",
        "      else:\n",
        "          actions = np.zeros((self.action_space, states.shape[1]))\n",
        "          actions[a, :] = 1\n",
        "\n",
        "\n",
        "      #print(states)\n",
        "      #print(actions)\n",
        "      states = np.concatenate((states, actions), axis=0)\n",
        "      return states\n",
        "\n",
        "\n",
        "def mean_squared_error(target, predicted, ax=0):\n",
        "    #print(target, \"|\", predicted)\n",
        "    return np.mean(np.square(target - predicted), axis=ax, keepdims=True)\n",
        "\n",
        "def MSE_grad(target, predicted):\n",
        "    try:\n",
        "        return (-2*np.sum((target - predicted), axis=0, keepdims=True))/target.shape[0]\n",
        "    except FloatingPointError:\n",
        "        print(target, predicted)\n",
        "        sys.exit()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = ValueEstimator((4,5,5,5,5), 2)\n",
        "next_state = np.array([[1, 0, 2],\n",
        "                       [1, 0, 4]])\n",
        "aaa = estimator.predict(estimator.addActionstoState(next_state))[0]\n",
        "print(aaa)\n",
        "print()\n",
        "np.reshape(aaa, (2, -1), 'F')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = ValueEstimator((1,5,5,5,5), 1)\n",
        "#print(estimator.layers)\n",
        "test = np.array([[0, 1, 0, 1],\n",
        "                  [0, 0, 1, 1]])\n",
        "target = np.array([[1, 2, 3, 4]])\n",
        "nums = np.array([np.random.uniform(-100, 100, 100)])\n",
        "\n",
        "for i in range(100000):\n",
        "    nums = np.array([np.random.uniform(-1000, 1000, 100)])\n",
        "    estimator.fit(nums, -nums, learning_rate=.001)\n",
        "    \"\"\"if (i==49999):\n",
        "        print(estimator.fit(nums, -nums, learning_rate=.01))\"\"\"\n",
        "    if (i%1000==0):\n",
        "        demo, _ = estimator.predict(nums)\n",
        "        print(mean_squared_error(-nums, demo, ax=None))\n",
        "\n",
        "\"\"\"for i in range(10000):\n",
        "    nums = np.array([np.random.uniform(0, 100, 100)])\n",
        "    estimator.fit(test, target, learning_rate=.01)\n",
        "    demo, _ = estimator.predict(test)\n",
        "    if (i%1000==0):\n",
        "        print(demo)\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#estimator = ValueEstimator((1,3,3), 1)\n",
        "\"\"\"test = np.array([[0],\n",
        "                  [0]])\n",
        "target = np.array([[1]])\"\"\"\n",
        "demo, _ = estimator.predict([[0.4]], print_layer_outputs=False)\n",
        "print(demo)\n",
        "#estimator.grad_check(test, target)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a, _ = estimator.predict(np.array([[6]]))\n",
        "print(a)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "    #Implented primarily by MiniQuark on stackoverflow\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size, combined=True):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        if combined:\n",
        "            indices.append(self.index-1)\n",
        "        return [self.buffer[index] for index in indices]\n",
        "\n",
        "\n",
        "def epsilon_greedy_policy(observation, estimator, epsilon, nA):\n",
        "    \"\"\"\n",
        "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
        "\n",
        "    Args:\n",
        "        estimator: An estimator that returns q values for a given state\n",
        "        epsilon: The probability to select a random action . float between 0 and 1.\n",
        "        nA: Number of actions in the environment.\n",
        "\n",
        "    Returns:\n",
        "        A function that takes the observation as an argument and returns\n",
        "        the probabilities for each action in the form of a numpy array of length nA.\n",
        "\n",
        "    \"\"\"\n",
        "    A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "    q_values = estimator.predict(estimator.addActionstoState(observation[:, np.newaxis]))[0].flatten()\n",
        "    best_action = []\n",
        "    for i, action in enumerate(q_values):\n",
        "                if action == max(q_values):\n",
        "                    best_action.append(i)\n",
        "\n",
        "    for a in best_action:\n",
        "        A[a] += (1.0 - epsilon)/len(best_action)\n",
        "    return A\n",
        "\n",
        "\n",
        "def reinforce(env, estimator, num_episodes, discount_factor=.99, start_epsilon=1.0, end_epsilon = 0.1, epsilon_steps=1000000, learning_rate = .00025, minibatch_size = 32, replay_memory_size = 1000000, target_update_freq = 100000, replay_memory = None):\n",
        "\n",
        "    #Initialize persistent variables\n",
        "    total_reward = 0\n",
        "    error_sum = 0\n",
        "    i_step = 0\n",
        "    epsilon_step = (start_epsilon - end_epsilon) / epsilon_steps\n",
        "    replay_memory = ReplayMemory(replay_memory_size) if (replay_memory == None) else replay_memory\n",
        "\n",
        "    #Create target estimator\n",
        "    target_estimator = copy.deepcopy(estimator)\n",
        "\n",
        "    #Fills replay memory\n",
        "    if replay_memory.size < minibatch_size:\n",
        "        state = env.reset()\n",
        "        for i in range(minibatch_size - replay_memory.size):\n",
        "            epsilon = end_epsilon + (epsilon_step*max(epsilon_steps - i_step, 0))\n",
        "            action_probs = epsilon_greedy_policy(state, estimator, epsilon, env.action_space.n)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            state = estimator.addActionstoState(state[:, np.newaxis], action)\n",
        "\n",
        "            replay_memory.append((state, next_state, reward, done))\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "            i_step+=1\n",
        "\n",
        "    #Start iterating through episodes\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "        if (i_episode%50==1):\n",
        "            print(f\"Episode {i_episode-1}, Reward: --{total_reward}--, Error: --{error_sum}--, Steps: {i_step}, Epsilon:{epsilon}\")\n",
        "            print()\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        total_reward = 0\n",
        "        error_sum = 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "\n",
        "        for t in itertools.count():\n",
        "            #TAKES A NEW STEP\n",
        "            #samples actions from policy\n",
        "            epsilon = end_epsilon + (epsilon_step*max(epsilon_steps - i_step, 0))\n",
        "            action_probs = epsilon_greedy_policy(state, estimator, epsilon, env.action_space.n)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "\n",
        "            #takes a step in the environment\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "\n",
        "            state = estimator.addActionstoState(state[:, np.newaxis], action)\n",
        "            replay_memory.append((state, next_state, reward, done))\n",
        "\n",
        "            if (i_episode % 50==0):\n",
        "                q_values_next = estimator.predict(estimator.addActionstoState(next_state[:, np.newaxis]))[0] if not done else 0\n",
        "                td_target = reward + discount_factor*np.max(q_values_next)\n",
        "\n",
        "                if np.isnan(td_target):\n",
        "                    print(td_target)\n",
        "                    raise OverflowError()\n",
        "\n",
        "                error_sum += mean_squared_error([[td_target]], estimator.predict(state)[0])\n",
        "                env.render()\n",
        "\n",
        "            #TRAINS FROM REPLAY MEMORY\n",
        "            #samples from replay memory\n",
        "            states, next_states, rewards, dones  = map(np.array, zip(*replay_memory.sample(minibatch_size)))\n",
        "            states = states.T\n",
        "            next_states = next_states.T\n",
        "\n",
        "            #predicts based on the next states\n",
        "            try:\n",
        "                q_values_nexts = np.reshape(estimator.predict(estimator.addActionstoState(next_states))[0], (env.action_space.n, -1), 'F')\n",
        "            except ValueError:\n",
        "                print(next_states)\n",
        "                raise ValueError()\n",
        "\n",
        "            q_values_nexts = np.where(dones, 0, np.amax(q_values_nexts, axis=0))\n",
        "            td_targets = rewards + discount_factor*q_values_nexts\n",
        "\n",
        "            #fits model\n",
        "            estimator.fit(state, td_targets[:, np.newaxis], learning_rate=learning_rate)\n",
        "\n",
        "            #updates target estimator to actual\n",
        "            if (i_step % target_update_freq) == 0:\n",
        "                print(\"Target updated!\")\n",
        "                target_estimator.layers = copy.deepcopy(estimator.layers)\n",
        "\n",
        "            #print(\"\\rStep {} @ Episode {}/{} ({})\".format(t, i_episode + 1, num_episodes, reward), end=\"\")\n",
        "\n",
        "            i_step += 1\n",
        "            if done:\n",
        "                if (i_episode % 50==0):\n",
        "                    print(estimator.predict(state)[0])\n",
        "                    #print(action_probs)\n",
        "                    error_sum = (error_sum/t)[0][0]\n",
        "                if (i_episode==num_episodes-1):\n",
        "                    pass\n",
        "                    #print(estimator.grad_check(state, np.array([[td_target]])))\n",
        "                break\n",
        "\n",
        "            state=next_state\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"CartPole-v1\")\n",
        "estimator = ValueEstimator((env.observation_space.shape[0]+env.action_space.n, 100, 200, 200, 200, 100), env.action_space.n)\n",
        "\n",
        "\n",
        "reinforce(env, estimator, 50000, discount_factor=.85, target_update_freq=50000, epsilon_steps=500000, replay_memory_size=200)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reinforce(env, estimator, 200000, discount_factor=.85, target_update_freq=50000, epsilon_steps=1, replay_memory_size=200)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(estimator.layers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "i = 0\n",
        "while True:\n",
        "  next_state, reward, done, _ = env.step(1)\n",
        "  env.render()\n",
        "  i+=1\n",
        "  if i%5 == 0:\n",
        "      print(estimator.predict(estimator.addActionstoState(next_state[:, np.newaxis]))[0])\n",
        "  if done:\n",
        "    break\n",
        "  state=next_state"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}